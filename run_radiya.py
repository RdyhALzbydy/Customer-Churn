#!/usr/bin/env python3
"""
Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù…Ø´Ø±ÙˆØ¹ Ø±Ø¶ÙŠØ©
ØªØ­Ù„ÙŠÙ„ ÙˆØªØ¯Ø±ÙŠØ¨ ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø§Ù†Ø³Ø­Ø§Ø¨
"""

import sys
import os
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
project_root = Path(__file__).parent
sys.path.append(str(project_root / "src"))

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
try:
    from radiya.data.loader import DataLoader
    from radiya.data.feature_engineer import FeatureEngineer
    from radiya.models.trainer import ModelTrainer
except ImportError as e:
    print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª: {e}")
    print("ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©")
    sys.exit(1)

# Ø¥Ø¹Ø¯Ø§Ø¯ MLflow
try:
    import mlflow
    mlflow.set_tracking_uri("sqlite:///radiya_experiments.db")
    mlflow.set_experiment("radiya_churn_prediction")
except ImportError:
    print("ØªØ­Ø°ÙŠØ±: MLflow ØºÙŠØ± Ù…ØªÙˆÙØ±ØŒ Ù„Ù† ÙŠØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªØ¬Ø§Ø±Ø¨")
    mlflow = None

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ø¬Ù„Ø§Øª
def setup_logging():
    """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ø¬Ù„Ø§Øª"""
    logs_dir = project_root / "logs"
    logs_dir.mkdir(exist_ok=True)
    
    log_file = logs_dir / f"radiya_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger("radiya_main")

def create_directories():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
    directories = [
        "data/raw", "data/processed", "models/saved_models", 
        "models/scalers", "reports/metrics", "reports/figures", "logs"
    ]
    
    for dir_path in directories:
        (project_root / dir_path).mkdir(parents=True, exist_ok=True)

def find_data_file():
    """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    possible_paths = [
        project_root / "data" / "raw" / "customer_churn_mini.json",
        project_root / "customer_churn_mini.json",
        Path("customer_churn_mini.json"),
        Path("data.json")
    ]
    
    for path in possible_paths:
        if path.exists():
            return str(path)
    
    return None

def print_header():
    """Ø·Ø¨Ø§Ø¹Ø© Ø±Ø£Ø³ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
    print("\n" + "="*80)
    print("                    Ù…Ø´Ø±ÙˆØ¹ Ø±Ø¶ÙŠØ© - Customer Churn Prediction")
    print("                         Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù†Ø³Ø­Ø§Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†")
    print("="*80)
    print(f"ÙˆÙ‚Øª Ø§Ù„Ø¨Ø¯Ø¡: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")

def run_data_analysis(data_path: str, logger):
    """ØªØ´ØºÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    logger.info("Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    print("\nğŸ“Š Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    print("-" * 50)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    loader = DataLoader(data_path)
    df = loader.load_data()
    
    # Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    summary = loader.get_data_summary()
    
    print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†: {summary['basic_stats']['unique_users']:,}")
    print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«: {summary['basic_stats']['clean_records']:,}")
    print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£ØºØ§Ù†ÙŠ Ø§Ù„Ù…Ø´ØºÙ„Ø©: {summary['basic_stats']['total_songs']:,}")
    print(f"Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ©: {summary['basic_stats']['date_range'][0].strftime('%Y-%m-%d')} Ø¥Ù„Ù‰ {summary['basic_stats']['date_range'][1].strftime('%Y-%m-%d')}")
    
    if 'subscription_levels' in summary:
        print("\nØªÙˆØ²ÙŠØ¹ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø§Ø´ØªØ±Ø§Ùƒ:")
        for level, count in summary['subscription_levels'].items():
            print(f"  {level}: {count:,} Ù…Ø³ØªØ®Ø¯Ù… ({count/summary['basic_stats']['unique_users']*100:.1f}%)")
    
    if 'page_distribution' in summary:
        print("\nØ£Ù‡Ù… 5 ØµÙØ­Ø§Øª:")
        top_pages = sorted(summary['page_distribution'].items(), key=lambda x: x[1], reverse=True)[:5]
        for page, count in top_pages:
            print(f"  {page}: {count:,}")
    
    return df, loader

def run_feature_engineering(df: pd.DataFrame, logger):
    """ØªØ´ØºÙŠÙ„ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª"""
    logger.info("Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª")
    print("\nğŸ”§ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª")
    print("-" * 50)
    
    engineer = FeatureEngineer()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª
    print("Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†...")
    features_df = engineer.create_features(df)
    
    print(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(features_df.columns)-1} Ù…ÙŠØ²Ø© Ù„Ù€ {len(features_df)} Ù…Ø³ØªØ®Ø¯Ù…")
    
    # ØªØ¬Ø±ÙŠØ¨ Ø·Ø±Ù‚ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø§Ù†Ø³Ø­Ø§Ø¨ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
    churn_methods = ['cancellation', 'downgrade', 'combined', 'inactivity']
    churn_results = {}
    
    print("\nØ·Ø±Ù‚ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø§Ù†Ø³Ø­Ø§Ø¨:")
    for method in churn_methods:
        try:
            churn_labels = engineer.define_churn(df, method=method)
            churn_rate = churn_labels['churned'].mean() * 100
            churn_count = churn_labels['churned'].sum()
            
            print(f"  {method}: {churn_count:,} Ù…Ù†Ø³Ø­Ø¨ ({churn_rate:.1f}%)")
            churn_results[method] = {
                'labels': churn_labels,
                'rate': churn_rate,
                'count': churn_count
            }
        except Exception as e:
            print(f"  {method}: Ø®Ø·Ø£ - {e}")
            churn_results[method] = None
    
    return features_df, engineer, churn_results

def run_model_training(features_df: pd.DataFrame, churn_results: dict, logger):
    """ØªØ´ØºÙŠÙ„ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
    logger.info("Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
    print("\nğŸ¤– Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
    print("-" * 50)
    
    all_results = {}
    best_overall_model = None
    best_overall_score = 0
    
    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø© ØªØ¹Ø±ÙŠÙ Ø§Ù†Ø³Ø­Ø§Ø¨
    for method_name, churn_data in churn_results.items():
        if churn_data is None:
            continue
            
        print(f"\nØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø©: {method_name}")
        print(f"Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø§Ù†Ø³Ø­Ø§Ø¨: {churn_data['rate']:.1f}%")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        ml_data = features_df.merge(churn_data['labels'], on='userId')
        X = ml_data.drop(['userId', 'churned'], axis=1)
        y = ml_data['churned']
        
        if len(y.unique()) < 2:
            print(f"ØªØ®Ø·ÙŠ {method_name} - Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØªÙ†ÙˆØ¹ ÙÙŠ Ø§Ù„ÙØ¦Ø§Øª")
            continue
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        trainer = ModelTrainer(random_state=42)
        method_results = trainer.train_all_models(X, y, experiment_name=method_name)
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        all_results[method_name] = {
            'trainer': trainer,
            'results': method_results,
            'churn_rate': churn_data['rate'],
            'data_shape': X.shape
        }
        
        # ØªØªØ¨Ø¹ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ø¹Ø§Ù…
        if trainer.best_model and trainer.best_score > best_overall_score:
            best_overall_score = trainer.best_score
            best_overall_model = {
                'method': method_name,
                'name': trainer.best_model['name'],
                'score': trainer.best_score,
                'trainer': trainer
            }
    
    return all_results, best_overall_model

def display_results(all_results: dict, best_overall_model: dict, logger):
    """Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""
    logger.info("Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
    print("\nğŸ“ˆ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
    print("=" * 80)
    
    # Ù…Ù„Ø®Øµ Ø´Ø§Ù…Ù„ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    summary_data = []
    
    for method_name, method_data in all_results.items():
        trainer = method_data['trainer']
        results = method_data['results']
        
        print(f"\nğŸ¯ Ø·Ø±ÙŠÙ‚Ø©: {method_name.upper()}")
        print(f"Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø§Ù†Ø³Ø­Ø§Ø¨: {method_data['churn_rate']:.1f}%")
        print(f"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {method_data['data_shape']}")
        print("-" * 60)
        
        # Ø¹Ø±Ø¶ Ù†ØªØ§Ø¦Ø¬ ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬
        for model_name, result in results.items():
            if 'metrics' in result:
                metrics = result['metrics']
                print(f"{model_name:15} | AUC: {metrics['auc']:.4f} | "
                      f"Precision: {metrics['precision']:.4f} | "
                      f"Recall: {metrics['recall']:.4f} | "
                      f"F1: {metrics['f1']:.4f}")
                
                summary_data.append({
                    'Method': method_name,
                    'Model': model_name,
                    'AUC': metrics['auc'],
                    'Precision': metrics['precision'],
                    'Recall': metrics['recall'],
                    'F1': metrics['f1'],
                    'CV_Mean': metrics['cv_mean'],
                    'CV_Std': metrics['cv_std']
                })
            else:
                print(f"{model_name:15} | Ø®Ø·Ø£: {result.get('error', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
    
    # Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ø¹Ø§Ù…
    print("\nğŸ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ø¹Ø§Ù…:")
    print("=" * 60)
    if best_overall_model:
        print(f"Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©: {best_overall_model['method']}")
        print(f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {best_overall_model['name']}")
        print(f"AUC Score: {best_overall_model['score']:.4f}")
        
        # Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
        save_paths = best_overall_model['trainer'].save_best_model(
            project_root / "models" / "saved_models"
        )
        print(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {save_paths['model_path']}")
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ ØµØ§Ù„Ø­")
    
    # Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    if summary_data:
        summary_df = pd.DataFrame(summary_data)
        summary_df = summary_df.sort_values('AUC', ascending=False)
        
        results_file = project_root / "reports" / "metrics" / f"results_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        summary_df.to_csv(results_file, index=False)
        print(f"\nØªÙ… Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {results_file}")
        
        # Ø¹Ø±Ø¶ Ø£ÙØ¶Ù„ 10 Ù†Ù…Ø§Ø°Ø¬
        print("\nğŸ“Š Ø£ÙØ¶Ù„ 10 Ù†Ù…Ø§Ø°Ø¬:")
        print("-" * 80)
        top_models = summary_df.head(10)
        for _, row in top_models.iterrows():
            print(f"{row['Method']:12} | {row['Model']:15} | AUC: {row['AUC']:.4f}")

def save_experiment_log(all_results: dict, best_overall_model: dict, data_path: str):
    """Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„ØªØ¬Ø±Ø¨Ø©"""
    
    experiment_log = {
        'timestamp': datetime.now().isoformat(),
        'data_path': data_path,
        'data_file_size_mb': Path(data_path).stat().st_size / 1024 / 1024,
        'total_methods_tested': len(all_results),
        'total_models_trained': sum(len(method_data['results']) for method_data in all_results.values()),
        'best_overall_model': {
            'method': best_overall_model['method'] if best_overall_model else None,
            'name': best_overall_model['name'] if best_overall_model else None,
            'score': best_overall_model['score'] if best_overall_model else 0
        },
        'method_results': {}
    }
    
    # ØªÙØ§ØµÙŠÙ„ ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø©
    for method_name, method_data in all_results.items():
        successful_models = [
            {
                'name': model_name,
                'auc': result['metrics']['auc'],
                'precision': result['metrics']['precision'],
                'recall': result['metrics']['recall'],
                'f1': result['metrics']['f1']
            }
            for model_name, result in method_data['results'].items() 
            if 'metrics' in result
        ]
        
        experiment_log['method_results'][method_name] = {
            'churn_rate': method_data['churn_rate'],
            'data_shape': method_data['data_shape'],
            'successful_models': len(successful_models),
            'best_model': max(successful_models, key=lambda x: x['auc']) if successful_models else None
        }
    
    # Ø­ÙØ¸ Ø§Ù„Ø³Ø¬Ù„
    log_file = project_root / "reports" / "metrics" / f"experiment_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(experiment_log, f, ensure_ascii=False, indent=2)
    
    print(f"ØªÙ… Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„ØªØ¬Ø±Ø¨Ø©: {log_file}")

def main():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø£ÙˆÙ„ÙŠ
    print_header()
    logger = setup_logging()
    create_directories()
    
    logger.info("Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ù…Ø´Ø±ÙˆØ¹ Ø±Ø¶ÙŠØ©")
    
    try:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data_path = find_data_file()
        if not data_path:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!")
            print("\nØ¶Ø¹ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø£Ø­Ø¯ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªØ§Ù„ÙŠØ©:")
            print("  - data/raw/customer_churn_mini.json")
            print("  - customer_churn_mini.json")
            return
        
        print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {data_path}")
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø±Ø§Ø­Ù„
        df, loader = run_data_analysis(data_path, logger)
        features_df, engineer, churn_results = run_feature_engineering(df, logger)
        all_results, best_overall_model = run_model_training(features_df, churn_results, logger)
        
        if all_results:
            display_results(all_results, best_overall_model, logger)
            save_experiment_log(all_results, best_overall_model, data_path)
        else:
            print("âŒ Ù„Ù… ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø£ÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")
        
        print("\n" + "="*80)
        print("âœ… Ø§Ù†ØªÙ‡Ù‰ ØªØ´ØºÙŠÙ„ Ù…Ø´Ø±ÙˆØ¹ Ø±Ø¶ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!")
        print("="*80)
        
        logger.info("Ø§Ù†ØªÙ‡Ù‰ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ù†Ø¬Ø§Ø­")
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {e}")
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()